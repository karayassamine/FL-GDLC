{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30ac35cb-1f5b-4a75-a0e4-b4c256ec99c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a47a2176-1cbe-413e-adeb-bc57de909fa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "multi_class: false\n",
      "n_clients: 10\n",
      "n_rounds: 20\n",
      "config_fit:\n",
      "  momentum: 0.9\n",
      "  local_epochs: 3\n",
      "  batch_size: 128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import flwr as fl\n",
    "import pickle\n",
    "from math import floor\n",
    "\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "from logging import INFO, DEBUG\n",
    "from flwr.common.logger import log\n",
    "\n",
    "\n",
    "\n",
    "with initialize(version_base=None, config_path=\"./config/\"):\n",
    "    cfg = compose(config_name='config.yaml')\n",
    "    print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "\n",
    "from src.data.dataset_info import datasets\n",
    "data_root = \"./datasets/partitions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25f7ea07-a9b3-48b5-8bb1-a0cf98b64e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 10\n",
    "BATCH_SIZE = 128\n",
    "folder_path = \"./datasets/preprocessed/\"\n",
    "clients_paths = [\n",
    "    folder_path + \"client_0.pkl\",\n",
    "    folder_path + \"client_1.pkl\",\n",
    "    folder_path + \"client_2.pkl\",\n",
    "    folder_path + \"client_3.pkl\",\n",
    "    folder_path + \"client_4.pkl\",\n",
    "    folder_path + \"client_5.pkl\",\n",
    "    folder_path + \"client_6.pkl\",\n",
    "    folder_path + \"client_7.pkl\",\n",
    "    folder_path + \"client_8.pkl\",\n",
    "    folder_path + \"client_9.pkl\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3485a114-abce-4789-888b-d70fde44e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def read_clients(folder_path, clients_paths, label_col, class_col, class_num_col, drop_columns, weak_columns):\n",
    "    test = pd.read_pickle(folder_path + \"test.pkl\")\n",
    "\n",
    "    if multi_class:\n",
    "        test[label_col] = test[class_num_col]\n",
    "\n",
    "\n",
    "   # \"\"test_by_class = {}\n",
    "   # classes = test[class_col].unique()\n",
    "   # for class_value in classes:\n",
    "   #     test_class = test[test[class_col] == class_value].copy()\n",
    "   #     test_class.drop(drop_columns, axis=1, inplace=True)\n",
    "   #     test_class.drop(weak_columns, axis=1, inplace=True)\n",
    "   #     test_class.reset_index(drop=True, inplace=True)\n",
    "\n",
    "   #    test_class_labels = test_class[label_col].to_numpy()\n",
    "   #    test_class = test_class.drop([label_col], axis=1).to_numpy()\n",
    "\n",
    "   #    test_by_class[class_value] = (test_class, test_class_labels)\"\"\n",
    "\n",
    "    test.drop(drop_columns, axis=1, inplace=True)\n",
    "    test.drop(weak_columns, axis=1, inplace=True)\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    test_labels = test[label_col].to_numpy()\n",
    "    test = test.drop([label_col], axis=1).to_numpy()\n",
    "    input_dim = test.shape[1]\n",
    "\n",
    "    client_data = []\n",
    "    for client_path in clients_paths:\n",
    "        client_data.append(pd.read_pickle(client_path))\n",
    "\n",
    "    for i in range(len(client_data)):\n",
    "\n",
    "        cdata = client_data[i]\n",
    "\n",
    "        if multi_class:\n",
    "            cdata[label_col] = cdata[class_num_col]\n",
    "       \n",
    "\n",
    "        cdata.drop(drop_columns, axis=1, inplace=True)\n",
    "        cdata.drop(weak_columns, axis=1, inplace=True)\n",
    "        cdata.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Split into train, validation, and test sets\n",
    "        c_train, c_test = train_test_split(cdata, test_size=0.1)\n",
    "\n",
    "        # Split c_train further into c_train and c_val\n",
    "        c_train, c_val = train_test_split(c_train, test_size=0.2)\n",
    "\n",
    "        # Extract labels and features for train, validation, and test\n",
    "        y_train = c_train[label_col].to_numpy()\n",
    "        x_train = c_train.drop([label_col], axis=1).to_numpy()\n",
    "\n",
    "        y_val = c_val[label_col].to_numpy()\n",
    "        x_val = c_val.drop([label_col], axis=1).to_numpy()\n",
    "\n",
    "        y_test = c_test[label_col].to_numpy()\n",
    "        x_test = c_test.drop([label_col], axis=1).to_numpy()\n",
    "\n",
    "        # Store in client_data: (x_train, y_train, x_val, y_val, x_test, y_test)\n",
    "        client_data[i] = (x_train, y_train, x_val, y_val, x_test, y_test)\n",
    "\n",
    "    return client_data, test, test_labels, test_by_class, input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d88a7e8c-1f64-43bd-b748-fa7ebe404544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes_set: {'Web Attack � XSS', 'DoS Hulk', 'Heartbleed', 'DoS Slowhttptest', 'Web Attack � Brute Force', 'Web Attack � Sql Injection', 'SSH-Patator', 'Infiltration', 'BENIGN', 'DoS GoldenEye', 'DDoS', 'DoS slowloris', 'FTP-Patator', 'Bot', 'PortScan'}\n",
      "==>> num_classes: 15\n",
      "==>> labels_names: {0: 'BENIGN', 1: 'Bot', 2: 'DDoS', 3: 'DoS GoldenEye', 4: 'DoS Hulk', 5: 'DoS Slowhttptest', 6: 'DoS slowloris', 7: 'FTP-Patator', 8: 'Heartbleed', 9: 'Infiltration', 10: 'PortScan', 11: 'SSH-Patator', 12: 'Web Attack � Brute Force', 13: 'Web Attack � Sql Injection', 14: 'Web Attack � XSS'}\n"
     ]
    }
   ],
   "source": [
    "# specifying the number of classes, since it is different from one dataset to another and also if binary or multi-class classification\n",
    "classes_set = {\"benign\", \"attack\"}\n",
    "labels_names = {0: \"benign\", 1: \"attack\"}\n",
    "num_classes = 2\n",
    "if cfg.multi_class:\n",
    "    classes_set = {\"BENIGN\", \"SSH-Patator\", \"FTP-Patator\", \"DoS slowloris\", \"Heartbleed\", \"DoS Slowhttptest\", \"DoS Hulk\", \"DoS GoldenEye\", \"Infiltration\", \"Web Attack � Brute Force\", \"Web Attack � XSS\", \"Web Attack � Sql Injection\", \"PortScan\", \"DDoS\", \"Bot\" }\n",
    "    labels_names = {0: 'BENIGN', 1: 'Bot', 2: 'DDoS', 3: 'DoS GoldenEye', 4: 'DoS Hulk', 5: 'DoS Slowhttptest', 6: 'DoS slowloris', 7: 'FTP-Patator', 8: 'Heartbleed', 9: 'Infiltration', 10: 'PortScan', 11: 'SSH-Patator', 12: 'Web Attack � Brute Force', 13: 'Web Attack � Sql Injection', 14: 'Web Attack � XSS'}\n",
    "    num_classes = len(classes_set)\n",
    "    \n",
    "labels_names = {int(k): v for k, v in labels_names.items()}\n",
    "\n",
    "print(f\"==>> classes_set: {classes_set}\")\n",
    "print(f\"==>> num_classes: {num_classes}\")\n",
    "print(f\"==>> labels_names: {labels_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc367e57-9d4a-4e12-9fad-84ce93fcaa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def create_nids_model(input_shape, num_classes):\n",
    "    model = Sequential(name='NIDS_CNN')\n",
    "    \n",
    "    # Feature extraction block 1\n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu', \n",
    "                     input_shape=input_shape, padding='same',\n",
    "                     kernel_regularizer=l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Feature extraction block 2\n",
    "    model.add(Conv1D(128, kernel_size=3, activation='relu', \n",
    "                     padding='same', kernel_regularizer=l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # Feature extraction block 3\n",
    "    model.add(Conv1D(256, kernel_size=2, activation='relu', \n",
    "                     padding='same', kernel_regularizer=l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Classification block\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "# model = create_nids_model(input_shape=(41, 1), num_classes=5)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "464baa2b-4da4-45a9-af22-50a72ef191ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_final = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f02dfca-4cb2-49a2-aa15-e87af4055e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlClient(fl.client.NumPyClient):\n",
    "    def __init__(self, context, model, x_train, y_train, x_val, y_val, x_test, y_test, input_dim):\n",
    "\n",
    "        self.context = context\n",
    "        self.model = model\n",
    "        self.x_train, self.y_train = x_train, y_train\n",
    "        self.x_val, self.y_val = x_val, y_val  \n",
    "        self.x_test, self.y_test = x_test, y_test\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        self.model.set_weights(parameters)\n",
    "        \n",
    "    def fit(self, parameters, config):\n",
    "\n",
    "        lr=0.001\n",
    "        self.model = create_keras_model(input_shape=self.input_dim, alpha=lr)\n",
    "        self.set_parameters(parameters, config)\n",
    "\n",
    "        history = self.model.fit(self.x_train,self.y_train\n",
    "                                validation_data=(self.x_val, self.y_val),  \n",
    "                                epochs=config[\"local_epochs\"],\n",
    "                                batch_size=config[\"batch_size\"],\n",
    "                                verbose=0)\n",
    "\n",
    "        return self.get_parameters(config), len(self.x_train), {k: v[-1] for k, v in history.history.items()}\n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters, config)\n",
    "        loss, accuracy = self.model.evaluate(self.X_val, self.y_val, 2, verbose=0)\n",
    "        return loss, len(self.X_val), {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7a3de7b-85e8-4207-b466-619514c9a4d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclient_fn\u001b[39m(context: \u001b[43mContext\u001b[49m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Client:\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Flower client representing a single organization.\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Load model\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Context' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_client_fn(data, labels):\n",
    "\n",
    "    def client_fn(context: Context):\n",
    "\n",
    "        client_id = int(context.node_config[\"partition-id\"])\n",
    "        print(f\"==>> client_id: {client_id}\")\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            data[client_id], labels[client_id], test_size=0.1, random_state=1)\n",
    "\n",
    "        print(f\"==>> X_train: {X_train}\")\n",
    "        print(f\"==>> X_val: {X_val}\")\n",
    "        \n",
    "        \n",
    "        model = create_keras_model(6)\n",
    "\n",
    "        return FLClient(\n",
    "            context, np.array(X_train), np.array(X_val), np.array(y_train), np.array(y_val), model, 1\n",
    "        ).to_client()\n",
    "\n",
    "    return client_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7671f77-cba5-4235-8870-fde475644371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ClientApp\n",
    "client_app = ClientApp(client_fn=generate_client_fn(\n",
    "    clients_data, clients_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79294111-d3eb-4d80-8492-8a6d194ac214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluate_fn(x_test_server, y_test_server):\n",
    "    def evaluate_fn(server_round: int, parameters, config):\n",
    "        eval_model = create_keras_model(input_shape=6)\n",
    "        eval_model.set_weights(parameters)\n",
    "\n",
    "        test_loss, test_acc = eval_model.evaluate(x_test_server, y_test_server,\n",
    "                                                  batch_size = 2)\n",
    "        \n",
    "        results_dict = {\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"round\": server_round\n",
    "        }\n",
    "\n",
    "        return results_dict\n",
    "\n",
    "    return evaluate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d3e07f-ae61-465b-b0ce-71dd3a441976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_on_fit_config():\n",
    "\n",
    "    def fit_config_fn(server_round: int):\n",
    "        return {\n",
    "            \"lr\": 0.001,\n",
    "            \"local_epochs\": 1,\n",
    "            \"batch_size\": 2,\n",
    "        }\n",
    "\n",
    "    return fit_config_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d756b2-7e04-4431-aaff-4db9db22cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_server_fn(data, labels):\n",
    "    def server_fn(context: Context):\n",
    "        strategy=fl.server.strategy.FedAvg(\n",
    "            fraction_fit=1.0,  # in simulation, since all clients are available at all times, we can just use `min_fit_clients` to control exactly how many clients we want to involve during fit\n",
    "            min_fit_clients=len(clients_data),  # number of clients to sample for fit()\n",
    "            fraction_evaluate=0.0,  # similar to fraction_fit, we don't need to use this argument.\n",
    "            min_evaluate_clients=0,  # number of clients to sample for evaluate()\n",
    "            min_available_clients=len(clients_data),\n",
    "            on_fit_config_fn=get_on_fit_config(),  # a function to execute to obtain the configuration to send to the clients during fit()\n",
    "            evaluate_fn=get_evaluate_fn(data, labels),\n",
    "        )\n",
    "        \n",
    "        return ServerAppComponents(\n",
    "            strategy=strategy,\n",
    "            config=ServerConfig(num_rounds=2)\n",
    "        )\n",
    "\n",
    "    return server_fn\n",
    "server_app = ServerApp(server_fn=generate_server_fn(data, labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50fbff4-c8f1-4ebb-8d0d-d0ddf06eb593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the resources each of your clients need\n",
    "# By default, each client will be allocated 1x CPU and 0x GPUs\n",
    "backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 0.0}}\n",
    "\n",
    "# When running on GPU, assign an entire GPU for each client\n",
    "if DEVICE == \"cuda\":\n",
    "    backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 1.0}}\n",
    "    # Refer to our Flower framework documentation for more details about Flower simulations\n",
    "    # and how to set up the `backend_config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcc4905-b20a-4c2b-bac4-b3b58d6edc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FedAvg strategy\n",
    "strategy = FedAvg(\n",
    "    fraction_fit=1.0,  # Sample 100% of available clients for training\n",
    "    fraction_evaluate=0.5,  # Sample 50% of available clients for evaluation\n",
    "    min_fit_clients=10,  # Never sample less than 10 clients for training\n",
    "    min_evaluate_clients=5,  # Never sample less than 5 clients for evaluation\n",
    "    min_available_clients=10,  # Wait until all 10 clients are available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c3429e-799a-4dda-a818-d3aef74dee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_data, test, test_labels, test_by_class, input_dim = read_clients(\n",
    "    folder_path, clients_paths, dataset.label_col, dataset.class_col, dataset.class_num_col, centralities_columns, pca_columns, dataset.drop_columns, dataset.weak_columns, cfg.multi_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c436a48e-f4ff-44f3-a763-a16a52cb1769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
