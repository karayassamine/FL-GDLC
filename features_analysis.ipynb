{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis for GDLC\n",
    "\n",
    "This notebook focuses on analyzing various features extracted from network traffic data to identify potential intrusions. Our goal is to understand the significance of each feature in the context of intrusion detection and to select the most relevant features for building a predictive model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import seaborn as sns\n",
    "\n",
    "from src.data.dataset_info import datasets\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# defining the thresholds\n",
    "var_threshold = 0.00\n",
    "corr_threshold = 0.75\n",
    "\n",
    "# specifying the dataset\n",
    "dataset = datasets[0]\n",
    "name = dataset.name\n",
    "print(\"dataset: {}\".format(name))\n",
    "path = \"./datasets/preprocessed/{}.pkl\".format(name)\n",
    "# graph_path = \"./datasets/preprocessed/graph_{}.gexf\".format(name)\n",
    "\n",
    "# loading the dataset as a Pandas dataframe\n",
    "df = pd.read_pickle(path)\n",
    "\n",
    "# We are only concerned with features used in training\n",
    "df.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "df.drop(dataset.label_col, axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VarianceThreshold object\n",
    "selector = VarianceThreshold(threshold=var_threshold)\n",
    "\n",
    "# Fit the selector to the data and transform the data\n",
    "data_filtered = selector.fit_transform(df)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features = df.columns[selector.get_support(indices=True)]\n",
    "\n",
    "dropped_features = [col for col in df.columns if col not in selected_features]\n",
    "\n",
    "# Create a new DataFrame with the selected features\n",
    "data_filtered = pd.DataFrame(df, columns=selected_features)\n",
    "\n",
    "# variances = np.var(df, axis=0)\n",
    "\n",
    "print(f\"==>> dropped_features by VarianceThreshold: {dropped_features}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the correlation matrix\n",
    "corr_matrix = data_filtered.corr()\n",
    "\n",
    "# plotting the heatmap of the matrix\n",
    "plt.figure(figsize=(25, 20))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm',\n",
    "            fmt='.2f', linewidths=.5)  # type: ignore\n",
    "plt.title(\"Correlation Matrix Heatmap in dataset {}\".format(dataset.name))\n",
    "# plt.savefig(fname=\"visualization/{}/correlation_matrix\".format(dataset.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the top correlated features\n",
    "upper = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "correlated_features = [\n",
    "    column for column in upper.columns if any(upper[column] > corr_threshold)]\n",
    "\n",
    "# Drop only one feature from each highly correlated pair\n",
    "features_to_remove = set()\n",
    "for feature in correlated_features:\n",
    "    correlated_with_feature = list(\n",
    "        upper.index[upper[feature] > corr_threshold])\n",
    "    for correlated_feature in correlated_with_feature:\n",
    "        if correlated_feature not in features_to_remove:\n",
    "            features_to_remove.add(correlated_feature)\n",
    "            # features_to_remove.add(np.random.choice([feature, correlated_feature]))\n",
    "\n",
    "\n",
    "# Drop the highly correlated features\n",
    "data_filtered = data_filtered.drop(features_to_remove, axis=1)  # type: ignore\n",
    "\n",
    "print(f\"==>> dropped_features by correlation: {features_to_remove}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"==>> final_features_names: {list(data_filtered.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_features.extend(list(features_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"==>> length of dropped_features: {len(dropped_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"==>> dropped_features: {dropped_features}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
